# -*- coding: utf-8 -*-
"""WaterPotability.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15MlbD33r6z08N9s8zrPUCjf6n9OgaTvm
"""

from google.colab import files
uploaded = files.upload()

import pandas as pd
import numpy as np

df = pd.read_csv('water_potability.csv')
df.head(5)

X = df.drop(columns='Potability',axis=1)
y = df['Potability'].values

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan , strategy = 'most_frequent')

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2 , random_state=42)

X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

from sklearn.impute import SimpleImputer
imputer = SimpleImputer(missing_values = np.nan , strategy = 'most_frequent')

y_train = imputer.fit_transform(y_train.reshape(-1,1))
y_test = imputer.transform(y_test.reshape(-1,1))

from sklearn.preprocessing import MinMaxScaler, StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""**Logistic Regression Algorithm**"""

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

"""**Decision Tree Algorithm**"""

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth = 4,random_state = 55,criterion = 'entropy')
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

"""**Random Forest Classifier**"""

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 200,random_state = 55,criterion='gini')
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_test,y_pred))

importances = model.feature_importances_
feature_names = X.columns
feature_importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})
feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)
print("Most Significant Features:")
feature_importance_df

X_scaled_df = pd.DataFrame(X, columns=X.columns)
selected_features = ["ph","Sulfate","Hardness","Solids"]
X_selected = X_scaled_df[selected_features]

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_selected,y,test_size=0.2,random_state=55)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators = 55,random_state = 55,criterion='gini',max_depth=6,min_samples_split=2, min_samples_leaf=1)
rf.fit(X_train,y_train)

y_pred = rf.predict(X_test)

print(accuracy_score(y_pred,y_test))

"""**Prediction Based on Random Values**"""

pH=3.22
H=248.07
solids=28749.71
sulfate=393.66
f = pd.DataFrame([[pH, H, solids, sulfate]], columns=['ph', 'Sulfate', 'Hardness', 'Solids'])
prediction = rf.predict(f)

print(prediction)
if prediction[0] == 1:
  print("Water is Potable")
else:
  print("Water is not Potable")

ph=8.00
H=203.40
solids=20401.52
sulfate=287.087
f = pd.DataFrame([[ph, H, solids, sulfate]], columns=['ph', 'Sulfate', 'Hardness', 'Solids'])
prediction = rf.predict(f)

print(prediction)
if prediction[0] == 1:
  print("Water is Potable")
else:
  print("Water is not Potable")

"""**PCA**"""

X = df.drop(columns=['Potability'])
y = df['Potability']
features = X.columns

from sklearn.impute import SimpleImputer
import numpy as np
si = SimpleImputer(missing_values=np.nan,strategy = 'mean')
X = si.fit_transform(X)

from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler()
X = sc.fit_transform(X)

from sklearn.decomposition import PCA
pca = PCA(n_components=X.shape[1])
pca.fit(X)
feature_importance = pd.DataFrame({
    'Feature': features,
    'Importance': abs(pca.components_[0])
}).sort_values(by='Importance', ascending=False)
print("Most Significant Features Based on First Principal Component:")
print(feature_importance)

X_scaled_df = pd.DataFrame(X, columns=features)
selected_features = ["Turbidity", "Solids", "Conductivity"]
X_selected = X_scaled_df[selected_features]

print("\nSelected Features:")
print(X_selected.head())

"""**Dimenionality Reduction and ML Algorithms**"""

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(X_selected,y,test_size=0.2,random_state=42)

from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_pred,y_test))

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(max_depth = 4,random_state = 55,criterion = 'entropy')
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_pred,y_test))

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators = 200,random_state = 55,criterion='gini')
model.fit(X_train,y_train)

y_pred = model.predict(X_test)

print(accuracy_score(y_pred,y_test))

"""**Accuracy has dropped. Hence we will stick with Supervised Approach**

Best accuracy = 0.7012195121951219 of Random Forest Algorithm
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
model = Sequential()
model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],)))
model.add(Dense(32, activation='relu'))
model.add(Dense(1, activation='sigmoid'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f'Neural Network Accuracy: {accuracy}')

